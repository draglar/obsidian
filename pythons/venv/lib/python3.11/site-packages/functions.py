import hashlib
import os
import subprocess
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import docx2txt
import PyPDF2

from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer

def summarize_text(text, num_sentences=50):
    parser = PlaintextParser.from_string(text, Tokenizer("english"))
    summarizer = LsaSummarizer()
    summary_sentences = summarizer(parser.document, num_sentences)
    return ' '.join(str(sentence) for sentence in summary_sentences)

def linux(command,output='yes'):
    output = subprocess.check_output(command,shell=True)
    output = output.decode('utf-8')
    variable = output.strip()
    if output == 'yes':
        return variable
    else:
        pass

def save_text_file(file_path, content):
    try:
        with open(file_path, 'w') as file:
            file.write(content)
        # print("Text file saved successfully!")
    except IOError:
        print("An error occurred while saving the text file.")

def generate_hash(input_str):
    m = hashlib.md5()
    m.update(input_str.encode())
    return m.hexdigest()+'.png'
'''
def check_and_generate_hash(text,dir_='fotos/',i=0):
    file_hash = generate_hash(text)
    if os.path.exists(dir_+file_hash):
        i += 1
        check_and_generate_hash(text+str(i),dir_,i)
    else:
        file_hash = generate_hash(text)
        name_ = os.path.splitext(file_hash)[0]
        return name_,file_hash
'''
def check_and_generate_hash(text, dir_='fotos/', i=0):
    file_hash = generate_hash(text)
    if os.path.exists(dir_ + file_hash):
        i += 1
        return check_and_generate_hash(text + str(i), dir_, i)
    else:
        name_ = os.path.splitext(file_hash)[0]
        return name_, file_hash


def tokenize_text(text):
    return nltk.word_tokenize(text)

def remove_stopwords(tokens):
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]
    return filtered_tokens

def lemmatize_tokens(tokens):
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]
    return lemmatized_tokens

def preprocess_texts(text):
    tokens = tokenize_text(text)
    filtered_tokens = remove_stopwords(tokens)
    lemmatized_tokens = lemmatize_tokens(filtered_tokens)
    preprocessed_text = " ".join(lemmatized_tokens)
    return preprocessed_text


def preprocess_file(file_path):
    _, file_extension = os.path.splitext(file_path)
    if file_extension == '.txt':
        # Preprocess text file
        processed = preprocess_txt(file_path)
    elif file_extension == '.md':
        # Preprocess markdown file
        processed = preprocess_md(file_path)
    elif file_extension == '.pdf':
        # Preprocess markdown file
        processed = preprocess_pdf(file_path)
    elif file_extension == '.html':
        # Preprocess HTML file
        processed = preprocess_html(file_path)
    elif file_extension == '.docx' or file_extension == '.doc':
        # Preprocess HTML file
        processed = preprocess_docx(file_path)
    else:
        print(f"Unsupported file format: {file_extension}")
    return processed
    

def preprocess_txt(file_path):
    # Add your text preprocessing logic here
    # Example: read the file and perform some operations
    with open(file_path, 'r') as file:
        text_data = file.read()
        # preprocessed_data = text_data.upper()  # Example: convert text to uppercase
        # Do further preprocessing or save the preprocessed data
        return preprocess_text(text)

def preprocess_docx(file_path):
    # Extract text from docx file
    text = docx2txt.process(file_path)
    
    # Tokenize the text
    tokens = word_tokenize(text)
    
    # Remove stopwords
    stop_words = set(stopwords.words("english"))
    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]
    
    # Lemmatize the words
    lemmatizer = WordNetLemmatizer()
    lem_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]
    
    # Join the tokens back into a preprocessed text string
    preprocessed_text = " ".join(lem_tokens)
    
    return preprocessed_text
# def preprocess_md(file_path):
#     # Add your markdown preprocessing logic here
#     # Example: read the file and perform some operations
#     with open(file_path, 'r') as file:
#         markdown_data = file.read()
#         # Perform the necessary preprocessing on the markdown content
#         preprocessed_data = markdown_data.replace('old_content', 'new_content')
        # Do further preprocessing or save the preprocessed data

def preprocess_md(file_path):
    # Tokenize the text into words
    with open(file_path, 'r') as file:
        markdown_text = file.read()

    words = word_tokenize(markdown_text)
    # Remove stopwords
    stop_words = set(stopwords.words("english"))
    words = [word for word in words if word.casefold() not in stop_words]
    
    # Lemmatize the words
    lemmatizer = WordNetLemmatizer()
    words = [lemmatizer.lemmatize(word) for word in words]
    
    # Join the words back into a preprocessed Markdown string
    preprocessed_markdown = " ".join(words)
    
    return preprocessed_markdown

def preprocess_html(file_path):
    # Add your HTML preprocessing logic here
    # Example: read the file and perform some operations
    with open(file_path, 'r') as file:
        html_data = file.read()
        # Perform the necessary preprocessing on the HTML content
        soup = BeautifulSoup(html_content, 'html.parser')
        text = soup.get_text()
        # Do further preprocessing or save the preprocessed data
        
    return preprocess_text(text)

def preprocess_pdf(file_path):
    # Open the PDF file
    with open(file_path, 'rb') as file:
        # Initialize a PDF reader object
        pdf_reader = PyPDF2.PdfReader(file)
        
        # Extract the text from each page
        text = ''
        for page in pdf_reader.pages:
            text += page.extract_text()
        
        # Tokenize the text
        tokens = word_tokenize(text)
        
        # Remove stopwords
        stop_words = set(stopwords.words('english'))
        filtered_tokens = [token for token in tokens if token.lower() not in stop_words]
        
        # Lemmatize the words
        lemmatizer = WordNetLemmatizer()
        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]
        
        # Return the preprocessed tokens
        # return lemmatized_tokens
        preprocessed_text = " ".join(lemmatized_tokens)
    
    return preprocessed_text

def process_text(text, n):
    sentences = nltk.sent_tokenize(text)
    paragraphs = []
    current_paragraph = ""
    
    for sentence in sentences:
        current_paragraph += sentence + " "
        if len(nltk.sent_tokenize(current_paragraph)) == n:
            paragraphs.append(current_paragraph.strip())
            current_paragraph = ""

    # Check if there are remaining sentences
    if current_paragraph:
        paragraphs.append(current_paragraph.strip())
    
    # If no paragraph is formed, return the maximum available sentences
    if not paragraphs:
        max_sentences = min(n, len(sentences))
        paragraphs.append(' '.join(sentences[:max_sentences]))
    
    return paragraphs
